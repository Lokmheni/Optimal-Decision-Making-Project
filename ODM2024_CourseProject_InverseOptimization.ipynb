{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ippQqrWAO0_v"
      },
      "source": [
        "## MGT-483 Optimal Decision Making 2024: Group Project\n",
        "\n",
        "In traditional linear programming (LP), the goal is to find an optimal solution of a given optimization model with known input parameters. Inverse optimization (IO), on the other hand, seeks an optimization model that explains a given optimal decision, thereby reversing the conventional optimization process. IO has applications in various fields such as economics, engineering, and operations research and thus plays an important role in decision-making processes. For example, if past decisions of an economic agent are observable, IO allows us to learn this agent's decision model. This model may then allow us to predict future decisions of the agent, which could impact our own decision problem. Therefore, understanding and advancing IO techniques may help us to improve decision-making processes across diverse application domains.\n",
        "\n",
        "This project is based on the paper [1], [which is available from here](https://arxiv.org/pdf/1511.04650.pdf) or from the [Management Science Webpage](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2017.2992).\n",
        "\n",
        "As a concrete example, consider the following situation. You are the manager of a manufacturing company entering a new market. The company aims to manufacture various products using a numer of raw materials. Your job is to negotiate the prices of these raw materials with the supplier. For this negotiation it would be helpful to know the prices that the supplier has offered to your competitor. Needless to say that your competitor has no interest in helping you by revealing this information. However, you may be able to estimate the prices offered to your competitor by using inverse optimization. To this end, assume that you know the production process used by your competitor. Assume also that you know the maximum amounts of the raw materials that can be ordered from the supplier over one planning cycle. This information allows you to construct the feasible set of the competitor's optimization problem (which is a manufacturing problem, that is, a linear program). Unfortunately, you do not know the competitor's objective function because it depends on the unknown prices for the raw materials. Fortunately, however, public companies are legally required to publish sales data, that is, you can observe your competitor's optimal production decisions. As we will see in this project, observing the optimal solution of a linear program with an unknown objective function allows us to construct an estimator for the objective function coefficients. Using this estimator, you can then (approximately) reconstruct the prices your competitor had to pay to the supplier for the raw materials. Now you are well prepared to enter negotiations with the supplier!\n",
        "\n",
        "Now let's start from the basics about how to find the objective function coefficients using inverse optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-sma4_LPYtv"
      },
      "source": [
        "## 1 Inverse Linear Programming: Formulation [20 points]\n",
        "Assume as ususal that $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{c} \\in \\mathbb{R}^n$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^m$.  We define the *linear programming* problem as:\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\mathbf{LP}(\\mathbf{c}): & \\quad \\underset{\\mathbf{x}}{\\text{minimize}} && \\quad \\mathbf{c}^\\top\\mathbf{x} \\\\\n",
        "& \\quad \\text{subject to} && \\quad \\mathbf{A}\\mathbf{x} \\ge \\mathbf{b}.\n",
        "\\end{alignedat}\n",
        "$$\n",
        "In the following we assume that $\\mathbf{A}$ and $\\mathbf{b}$ are known. However, we are not given the cost vector $\\mathbf{c}$. Instead, we have access to a decision $\\mathbf{x}^0 \\in \\mathbb{R}^n$, which is known to be an optimal or a near-optimal solution of $\\mathbf{LP}(\\mathbf{c})$. Inverse optimization aims to determine a vector $\\mathbf c$ that explains the observed decision $\\mathbf x^0$. More precisely, given $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf{x}^0$, inverse optimzation seeks a $\\mathbf c$ that makes $\\mathbf{x}^0$ optimal in $\\mathbf{LP}(\\mathbf{c})$. Note that if $\\mathbf c=\\mathbf 0$, then every feasible solution of $\\mathbf{LP}(\\mathbf{c})$ is optimal. Thus, it makes sense to search for a normalized cost vector that satisfies $\\|\\mathbf{c}\\|_1 = 1$. This specification gives rise to the following *inverse optimization* problem.\n",
        "<!-- In the data-driven regime, it is possible that the observed decision $\\mathbf x^0$ is not an exact optimal solution but an approximate one due to data noise. Thus we denote by $\\mathbf x^0$ as the observed solution. We consider $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf{x}^0$ to be given; thus, they form the *data* that we use to infer the cost vector $\\mathbf{c}$. We start with what we refer to as the classical *inverse* optimization problem, which finds a cost vector $\\mathbf{c}$ such that $\\mathbf{x}^0$ is optimal for $\\mathbf{IP}(\\mathbf{c})$: -->\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\mathbf{IO}(\\mathbf{x}^0): & \\quad \\underset{\\mathbf{y},\\mathbf{c}}{\\text{minimize}} && \\quad 0 \\\\\n",
        "& \\quad \\text{subject to} && \\quad \\mathbf{A}^\\top\\mathbf{y} = \\mathbf{c} \\\\\n",
        "& && \\quad \\mathbf{c}^\\top\\mathbf{x}^0 = \\mathbf{b}^\\top\\mathbf{y} \\\\\n",
        "& && \\quad \\|\\mathbf{c}\\|_1 = 1 \\\\\n",
        "& && \\quad \\mathbf{y} \\ge \\mathbf{0}\n",
        "\\end{alignedat}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Fdw4S2PZq3"
      },
      "source": [
        "### 1.1 Interpretation of $\\mathbf{IO}(\\mathbf{x}^0)$ [10 points]\n",
        "Show that if $(\\mathbf y^*, \\mathbf c^*)$ is an optimal solution of $\\mathbf{IO}(\\mathbf{x}^0)$ and if $\\mathbf x^0$ is feasible (that is, $\\mathbf{A}\\mathbf{x}^0 \\ge \\mathbf{b}$), then $\\mathbf x^0$ is an optimal solution of $\\mathbf{LP}(\\mathbf{c}^*)$. *Hint:* Use the weak duality theorem.\n",
        "<!-- Can you explain why one would formuate the inverse problem as $\\mathbf{IO}(\\mathbf{x^0})$ (including the objective, limitations, and decision variables). Show that $\\mathbf{c}$ obtained by solving $\\mathbf{IO}(\\mathbf{x^0})$  is optimal for $\\mathbf{IP}(\\mathbf{c})$? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q34hbGREPcvW"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNMq99jJPhpG"
      },
      "source": [
        "### 1.2 Infeasibility of $\\mathbf{IO}(\\mathbf{x}^0)$ [10 points]\n",
        "Explain why $\\mathbf{IO}(\\mathbf{x}^0)$ fails to be a linear program. Show that if we know in advance that $\\mathbf c \\geq \\mathbf 0$, however, then $\\mathbf{IO}(\\mathbf{x}^0)$ simplifies to a linear program. Solve $\\mathbf{IO}(\\mathbf{x}^0)$ using CVXPY for the $\\mathbf A$, $\\mathbf b$ and $\\mathbf x^0$ given below. Report the result. Is $\\mathbf{IO}(\\mathbf{x}^0)$ feasible? If not, why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyuhkX-vOtpU"
      },
      "outputs": [],
      "source": [
        "# Import packages.\n",
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "A = np.array([[2, 5],\n",
        "              [2, -3],\n",
        "              [2, 1],\n",
        "              [-2, -1]])\n",
        "b = np.array([10, -6, 4, -10])\n",
        "x0 = np.array([2.5, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it7Y86KHPmmV"
      },
      "outputs": [],
      "source": [
        "# Define variables, objective, and constraints\n",
        "your code here\n",
        "\n",
        "# Solve the problem\n",
        "your code here\n",
        "\n",
        "# Print the result if you want\n",
        "your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNFv032wPxJv"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq11tghYPzkv"
      },
      "source": [
        "## 2 Generalized Inverse Linear Optimization [15 points]\n",
        "We have seen that even if $\\mathbf x^0$ is a feasible solution for the linear programming problem, it is possible that the inverse optimization problem $\\mathbf{IO}(\\mathbf{x}^0)$ is infeasible. This can happen when $\\mathbf x^0$ is not an exact optimizer of the linear programming problem but a noisy observation of an exact optimizer. We thus introduce the following generalized formulation of the inverse optimization problem.\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\textbf{GIO}(\\mathbf{x}^0): & \\quad \\underset{\\mathbf{y},\\mathbf{c},\\boldsymbol{\\epsilon}}{\\text{minimize}} && \\quad {\\|\\boldsymbol{\\epsilon}\\|_p} \\\\\n",
        "&\\quad\\text{subject to}  && \\quad \\mathbf{A}^\\top\\mathbf{y} = \\mathbf{c}\\\\\n",
        "& && \\quad \\mathbf{c}^\\top(\\mathbf{x}^0 - \\boldsymbol{\\epsilon}) = \\mathbf{b}^\\top\\mathbf{y}\\\\\n",
        "& && \\quad \\|\\mathbf{c}\\|_1 = 1 \\\\\n",
        "& && \\quad \\mathbf{y} \\ge \\mathbf{0}\n",
        "\\end{alignedat}\n",
        "$$\n",
        "Here, the new auxiliary decision vector $\\boldsymbol \\epsilon$ captures the observation noise. The problem aims to find a cost vector $\\mathbf c$ for which the de-noised decision $\\mathbf x^0-\\boldsymbol \\epsilon$ is optimal in $\\mathbf{LP}(\\mathbf c)$, while keeping the noise as small as possible with\n",
        "respect to the $p$-norm for some $p\\geq 1$. This alternative model is guaranteed to be feasible. However, this problem still involves two non-linear constraints: $\\mathbf{c}^\\top(\\mathbf{x}^0 - \\boldsymbol{\\epsilon}) = \\mathbf{b}^\\top\\mathbf{y}$ and $\\|\\mathbf{c}\\|_1 = 1$. It remains unclear how to solve the problem directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RBsb0P6P1BH"
      },
      "source": [
        "### 2.1 Closed-form Solution\n",
        "Luckily, problem $\\mathbf{GIO}(\\mathbf x^0)$ admits a closed-form solution. In Reference [1], you will find the following theorem.\n",
        "\n",
        "\n",
        "#### Theorem 1.\n",
        "Given $\\mathbf{x}^0$ such that $\\mathbf{A}\\mathbf{x}^0\\geq \\mathbf b$, where $\\mathbf{A}$ is a full-rank matrix, an optimal solution to $\\mathbf{GIO}(\\mathbf{x}^0)$ is given by\n",
        "$$\n",
        "\\begin{equation}\n",
        "(\\mathbf y^*,\\mathbf c^*,\\boldsymbol \\epsilon^*) = \\left(\\frac{\\mathbf{e}_{i^*}}{\\|\\mathbf{a}_{i^*}\\|_1},\\frac{\\mathbf{a}_{i^*}}{\\|\\mathbf{a}_{i^*}\\|_1},\\frac{\\mathbf{a}_{i^*}^\\top \\mathbf{x}^0-b_{i^*}}{\\|\\mathbf{a}_{i^*}\\|_q}\\mathbf{v}^*\\right)\n",
        "\\end{equation},\n",
        "$$\n",
        "where $\\mathbf{a}_i$ defines the $i$-th row of $\\mathbf{A}$, $\\mathbf{e}_i$ is the $i$-th standard basis vector whose entries are all 0 except for the $i$-th component, which equals 1. In addition, $q$ satisfies $1/p+1/q=1$, where $p$ defines the error norm in $\\mathbf{GIO}(\\mathbf{x}^0)$, while\n",
        "$$i^*\\in \\underset{i\\in 1,...,m}{\\text{argmin}}\\{(\\mathbf{a}^\\top_i\\mathbf{x}^0-b_i)/\\|\\mathbf{a}_i\\|_q\\}  \\quad \\text{and}\\quad \\mathbf{v}^*\\in \\underset{\\|\\mathbf{v}\\|_p=1}{\\text{argmin}}\\ \\mathbf{a}^\\top_{i^*}\\mathbf{v}.$$\n",
        "Furthermore, the optimal objective value of $\\mathbf{GIO}(\\mathbf{x}^0)$ is $\\ \\|\\boldsymbol{\\epsilon}^*\\|_p=(\\mathbf{a}^\\top_{i^*}\\mathbf{x}^0-b_{i^*})/\\|\\mathbf{a}_{i^*}\\|_q.$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N1h3lNWP3Ie"
      },
      "source": [
        "### 2.2 Interpretation of $\\mathbf{GIO}(\\mathbf{x}^0)$ [15 points]\n",
        "\n",
        "Assume that $p=1$. Solve $\\mathbf{GIO}(\\mathbf{x}^0)$ using Theorem 1 for the same data $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf x^0$ that you used in Section 1.2. Denote the resulting optimal cost vector by $\\hat{\\mathbf{c}}$.\n",
        "\n",
        "Next, solve $\\mathbf{LP}(\\hat{\\mathbf{c}})$ with CVXPY. Compare the resulting optimizer with $\\mathbf x^0$, and compare the resulting optimal value with $\\hat{\\mathbf c}^\\top \\mathbf x^0$. Solve the same question again under the assumption that $p=\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBl7oEIaP5xN"
      },
      "outputs": [],
      "source": [
        "# solve GIO(x0)\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og47G6xoQFxm"
      },
      "outputs": [],
      "source": [
        "# solve LP(c_hat)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnFDt6qTQIZu"
      },
      "outputs": [],
      "source": [
        "# compare/print\n",
        "your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxpW5vQbQSLu"
      },
      "source": [
        "## 3 Absolute and Relative Duality Gaps [35 points]\n",
        "Problem $\\textbf{GIO}_a(\\mathbf{x}^0)$ guarantees feasibility by adding a perturbation to $\\mathbf x^0$. However, the resulting problem is not a linear program. One can also formulate an alternative generalized inverse optimization model that allows for a non-vanishing duality gap as shown below.\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\textbf{GIO}_a(\\mathbf{x}^0): & \\quad \\underset{\\mathbf{y},\\mathbf{c},\\epsilon_a}{\\text{minimize}} && \\quad \\epsilon_a \\\\\n",
        " & \\quad\\text{subject to} && \\quad \\mathbf{A}^\\top\\mathbf{y} = \\mathbf{c} \\\\\n",
        "& && \\quad \\mathbf{c}^\\top\\mathbf{x}^0 = \\mathbf{b}^\\top\\mathbf{y}+\\epsilon_a \\\\\n",
        "& && \\quad \\|\\mathbf{c}\\|_1 = 1 \\\\\n",
        "& && \\quad \\mathbf{y} \\ge \\mathbf{0}\n",
        "\\end{alignedat}\n",
        "$$\n",
        "Here, the new auxiliary variable $\\epsilon_a$ captures indeed the absolute duality gap, which we aim to minimize. Note that problem $\\textbf{GIO}_a(\\mathbf{x}^0)$ is again guaranteed to be feasible and reduces to a linear program when we add the constraint $\\mathbf c\\geq \\mathbf 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZLJ5UduQWE1"
      },
      "source": [
        "### 3.1 Interpretation of $\\mathbf{GIO}_a(\\mathbf{x}^0)$ [20 points]\n",
        "\n",
        "Solve problem $\\mathbf{GIO}_a(\\mathbf{x}^0)$ using CVXPY for the same data $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf x^0$ as in Section 1.2, and assume that $\\mathbf{c}\\geq \\mathbf 0$. Denote the optimal cost vector by $\\hat{\\mathbf{c}}$. Solve $\\mathbf{LP}(\\hat{\\mathbf{c}})$ and compare the resulting optimal solution with $\\mathbf x^0$, and compare its optimal value with $\\hat{\\mathbf c}^\\top \\mathbf x^0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKS6RXh6Qcim"
      },
      "outputs": [],
      "source": [
        "# solve GIO_a(x0)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyNpYc9eQd_O"
      },
      "outputs": [],
      "source": [
        "# solve LP(c_hat)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exoNktGmQf0P"
      },
      "outputs": [],
      "source": [
        "# compare/ print\n",
        "your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaTOYLU_QmQX"
      },
      "source": [
        "Solve problem $\\mathbf{GIO}_a(\\mathbf{x}^0)$ again using CVXPY for the same data $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf x^0$ as in Section 1.2. This time, however, do not assume that $\\mathbf{c}\\geq \\mathbf 0$. In this case, $\\mathbf{GIO}_a(\\mathbf{x}^0)$ is not a linear program. Show that $\\mathbf{GIO}_a(\\mathbf{x}^0)$ can be solved by solving four different linear programs, one for each facet of the 1-norm unit ball, such that the optimal value of $\\mathbf{GIO}_a(\\mathbf{x}^0)$ coincides with the smallest of the optimal values of the four linear programs. The four linear programs can be obtained by restricting the signs of all coordinates of the 2-dimensional vector $\\mathbf{c}$.\n",
        "\n",
        "Denote the optimal cost vector by $\\hat{\\mathbf{c}}$. Solve $\\mathbf{LP}(\\hat{\\mathbf{c}})$. Compare the resulting optimal solution with $\\mathbf x^0$, and compare its optimal value with $\\hat{\\mathbf c}^\\top \\mathbf x^0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cyCSNyyQpfu"
      },
      "outputs": [],
      "source": [
        "# solve GIO_a(X0)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IanEZJ5vQqxm"
      },
      "outputs": [],
      "source": [
        "# solve LP(c)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkOs18oYQtwH"
      },
      "outputs": [],
      "source": [
        "# compare/ print\n",
        "your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlKMoUsRQwyX"
      },
      "source": [
        "### 3.2 Relative Duality Gap [15 points]\n",
        "\n",
        "Instead of minimizing the absolute duality gap, we can also formulate in inverse optimization problem that minimizes the relative duality gap as shown below.\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\textbf{GIO}_r(\\mathbf{x}^0): & \\quad \\underset{\\mathbf{y},\\mathbf{c},\\epsilon_r}{\\text{minimize}} && \\quad |\\epsilon_r - 1| \\\\\n",
        "&\\quad \\text{subject to}  && \\quad \\mathbf{A}^\\top\\mathbf{y} = \\mathbf{c} \\\\\n",
        "& && \\quad \\mathbf{c}^\\top\\mathbf{x}^0 = \\epsilon_r\\mathbf{b}^\\top\\mathbf{y} \\\\\n",
        "& && \\quad \\|\\mathbf{c}\\|_1 = 1 \\\\\n",
        "& && \\quad \\mathbf{y} \\ge \\mathbf{0}\n",
        "\\end{alignedat}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjdIsaIQQz-m"
      },
      "source": [
        "### Reformulation of $\\textbf{GIO}_r(\\mathbf{x}^0)$\n",
        "Explain why problem $\\mathbf{GIO}_r(\\mathbf{x}^0)$ fails to be a linear program even if we include the constraint $\\mathbf c\\geq \\mathbf 0$. Show that the optimal solution of problem $\\mathbf{GIO}_r(\\mathbf{x}^0)$ can be obtained from the optimal solution of the following problem by rescaling.\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "\\textbf{GIO}_{r}'(\\mathbf{x}^0): & \\quad \\underset{\\mathbf{y},\\mathbf{c},\\epsilon_r}{\\text{minimize}} && \\quad |\\epsilon_r - 1| \\\\\n",
        "&\\quad \\text{subject to}  && \\quad \\mathbf{A}^\\top\\mathbf{y} = \\mathbf{c} \\\\\n",
        "& && \\quad \\mathbf{c}^\\top\\mathbf{x}^0 = \\epsilon_r\\mathbf{b}^\\top\\mathbf{y} \\\\\n",
        "& && \\quad |\\mathbf{b}^\\top\\mathbf{y}| = 1 \\\\\n",
        "& && \\quad \\mathbf{y} \\ge \\mathbf{0}\n",
        "\\end{alignedat}\n",
        "$$\n",
        "Show that $\\textbf{GIO}_{r}'(\\mathbf{x}^0)$ can be sovled by solving two separate linear programs corresponding to $\\mathbf{b}^\\top \\mathbf{y}=+1$ and $\\mathbf{b}^\\top \\mathbf{y}=-1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc2iCeBvQ11O"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvOz5bq-Q63G"
      },
      "source": [
        "Solve problem $\\mathbf{GIO}_r(\\mathbf{x}^0)$ using CVXPY for the same data $\\mathbf{A}$, $\\mathbf{b}$ and $\\mathbf x^0$ as in Section 1.2. Denote the optimal cost vector by $\\hat{\\mathbf{c}}$. Solve $\\mathbf{LP}(\\hat{\\mathbf{c}})$. Compare the resulting optimal solution with $\\mathbf x^0$, and compare its optimal value with $\\hat{\\mathbf c}^\\top \\mathbf x^0$.  Note that you do not need to assume that $\\mathbf{c}\\geq \\mathbf 0$. Is $\\frac{\\mathbf{x}^0}{\\epsilon_r}$ feasible or optimal in $\\mathbf{LP(\\hat{c})}$? Please explain why it is feasible or optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RacQpV_Q9Oe"
      },
      "outputs": [],
      "source": [
        "# solve GIO_r\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjVDb0nXRBAn"
      },
      "outputs": [],
      "source": [
        "# solve LP(c_hat)\n",
        "your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xFVxREtRErH"
      },
      "outputs": [],
      "source": [
        "# compare/ check feasibility and optimalit/ print\n",
        "your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmCI3WB_RSDn"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJZbHniRTBO"
      },
      "source": [
        "## 4 Applications to Price Negotiation [15 points]\n",
        "\n",
        "We now come back to the motivating example of a manager who has to negotiate the prices for raw materials with the supplier. You know that your competitor uses the same raw materials M1, M2 and M3 for producing four products P1, P2, P3 and P4. Assume that you know the production process used by your competitor, that is, you know a matrix $\\mathbf A\\in\\mathbb R^{3\\times 4}$, where $\\mathbf A_{ij}$ denotes the number of units of raw material Mi that are needed to prodce one unit of product Pj. Assume also that you know the vector $\\mathbf b\\in\\mathbb R^3$ of maximum amounts of the raw materials that can be ordered from the supplier over one planning cycle. Hence, the feasible set of the competitor's optimization problem is given by $\\{\\mathbf x\\in\\mathbb R^3_+:\\mathbf A\\mathbf x\\leq \\mathbf b\\}$. You also know the vector of prices $\\mathbf p\\in\\mathbb R^4$, at which the competitor sells the products. But we have only limited information about the vector $\\mathbf q\\in\\mathbb R^3$ of prices at which the competitor bought the raw materials from the supplier. More precisely, we know that $\\mathbf q_3=10$, but we do not know $\\mathbf q_1$ and $\\mathbf q_2$. Overall, you thus know that the competitor's objective is to maximize $\\mathbf p^\\top\\mathbf x - \\mathbf q^\\top \\mathbf A\\mathbf x$. Finally, you observe the competitor's production quantities $\\mathbf x^0\\in\\mathbb R^4$, because sales data is public knowlege. The following tables contain the entries of the matrix $\\mathbf A$ and the vectors $\\mathbf b$, $\\mathbf p$ and $\\mathbf x^0$.\n",
        "\n",
        "<center>\n",
        "\n",
        "|  $\\mathbf A_{ij}$  | P1 | P2 | P3 | P4 |\n",
        "| ---- | ---- | ---- | ---- |---- |\n",
        "| M1 | 3 | 1 | 0 | 0 |\n",
        "| M2 | 0 | 1 | 2 | 0 |\n",
        "| M3 | 1 | 1 | 1 | 2 |\n",
        "\n",
        "\n",
        "\n",
        "|  $\\mathbf p_j$  | P1 | P2 | P3 | P4 |\n",
        "| ---- | ---- | ---- | ---- |---- |\n",
        "| Price ($/unit) | 100 | 50 | 50 | 40 |\n",
        "\n",
        "\n",
        "\n",
        "|  $\\mathbf b_i$  | M1 | M2 | M3 |\n",
        "| ---- | ---- | ---- | ---- |\n",
        "| Limit (units) | 15 | 10 | 15 |\n",
        "\n",
        "|  $\\mathbf x^0_j$  | P1 | P2 | P3 | P4 |\n",
        "| ---- | ---- | ---- | ---- |---- |\n",
        "| Production quantity (units) | 5.0 | 0.0 | 5.0 | 2.5 |\n",
        "\n",
        "</center>\n",
        "\n",
        "Formulate the competitor's optimization problem. Assuming that $\\mathbf x^0$ is an optimal solution of this problem, formulate an inverse optimization problem akin to $\\textbf{GIO}_{r}'(\\mathbf{x}^0)$ that you can use to infer the gradient $\\mathbf c=\\mathbf p-\\mathbf A^\\top \\mathbf q$ of the competitor's objective function. Solve the resulting inverse optimization problem numerically using CVXPY to obtain an estimator $\\hat{\\mathbf c}$ of the objective function gradient. Use this estimator to build a least-square problem that estimates the unknown prices for the raw materials M1 and M2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZrKYVbYRXUO"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpIlmp1QRdIO"
      },
      "outputs": [],
      "source": [
        "# define data\n",
        "your code here\n",
        "\n",
        "# solve GIO_r\n",
        "your code here\n",
        "\n",
        "# compute the price and print result\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7VVmqNjR9CX"
      },
      "source": [
        "## 5 Stochastic Inverse Optimization [15 points]\n",
        "Consider the matrix $\\mathbf{A}$ and the vectors $\\mathbf{b}$ and $\\mathbf{c}_0$ given below. In addition, assume that $\\mathbf c$ follows a two-dimensional Gaussian distribution with mean $\\mathbf{c}_0$ and covariance matrix $100*I_2$, where $I_2$ denotes the $2\\times 2$ identity matrix. Solve $\\mathbf{LP}(\\mathbf c_0)$ to obtain $\\mathbf x^0$. Solve $\\mathbf{GIO}(\\mathbf x^0)$ with $\\mathbf{x}_0$ to obtain $\\hat{\\mathbf c}$ as benchmark. Generate $n=10$ i.i.d. samples from the distribution of $\\mathbf c$. Solve the empirical problem shown below to obtain $\\mathbf{x}_{n}^0$.\n",
        "$$\n",
        "\\begin{alignedat}{2}\n",
        "& {\\text{minimize}_x} && \\quad \\sum_{i=1}^n \\mathbf{c}_i^\\top \\mathbf{x} \\\\\n",
        "& \\text{subject to}  && \\quad \\mathbf{A}\\mathbf{x}\\geq \\mathbf{b}\\\\\n",
        "\\end{alignedat}\n",
        "$$\n",
        "\n",
        "Solve $\\mathbf{GIO}(\\mathbf x_{n}^0)$ with $\\mathbf{c}\\geq \\mathbf 0$ and $p=\\infty$ to recover an estimator $\\hat{\\mathbf{c}}_{10}$. Compare $\\hat{\\mathbf{c}}_{10}$ against $\\hat{\\mathbf{c}}$. Repeat the same experiment for $n=100$ and $n=500$, and compare the resulting estimators $\\hat{\\mathbf{c}}_{n}$ against $\\hat{\\mathbf{c}}$. Plot the distance $\\|\\hat{\\mathbf{c}}-\\hat{\\mathbf{c}}_{n}\\|_2$ against the number of samples $n$, which should range from $1$ to $1{,}000$ in steps of $50$. Interpret your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxEtsxTQR-6u"
      },
      "outputs": [],
      "source": [
        "# Given Data A, b\n",
        "m = 30\n",
        "n = 20\n",
        "A = np.array([[13.0,71.0,-28.0,-50.0,1.0,96.0,5.0,4.0,40.0,24.0,-73.0,89.0,-0.0,76.0,-44.0,-37.0,-33.0,53.0,-80.0,-68.0],\n",
        "              [68.0,-98.0,-91.0,-7.0,-33.0,-73.0,48.0,63.0,-66.0,-29.0,12.0,64.0,-94.0,99.0,-69.0,21.0,76.0,-47.0,-73.0,59.0],\n",
        "              [89.0,-13.0,6.0,-34.0,-79.0,-27.0,60.0,47.0,-100.0,-58.0,30.0,46.0,26.0,4.0,-65.0,-5.0,17.0,77.0,57.0,40.0],\n",
        "              [65.0,74.0,-8.0,60.0,-44.0,12.0,69.0,6.0,-14.0,6.0,77.0,91.0,39.0,26.0,7.0,80.0,77.0,-60.0,-25.0,69.0],\n",
        "              [59.0,40.0,83.0,54.0,11.0,-94.0,56.0,14.0,40.0,6.0,44.0,25.0,-60.0,-23.0,-3.0,-75.0,44.0,17.0,-76.0,96.0],\n",
        "              [-91.0,86.0,9.0,-74.0,41.0,68.0,67.0,43.0,1.0,75.0,79.0,-72.0,32.0,22.0,30.0,-22.0,-17.0,97.0,5.0,-79.0],\n",
        "              [63.0,27.0,-96.0,-76.0,40.0,23.0,-38.0,42.0,68.0,11.0,94.0,51.0,-30.0,-8.0,-74.0,-23.0,-85.0,-46.0,-23.0,-61.0],\n",
        "              [-84.0,0.0,-20.0,23.0,-26.0,50.0,-44.0,52.0,91.0,-18.0,13.0,-73.0,62.0,48.0,83.0,-99.0,-24.0,83.0,68.0,-20.0],\n",
        "              [59.0,54.0,6.0,66.0,-39.0,-49.0,-86.0,28.0,10.0,59.0,-33.0,-16.0,-44.0,9.0,25.0,-47.0,-23.0,6.0,68.0,0.0],\n",
        "              [80.0,70.0,1.0,2.0,-87.0,-81.0,19.0,-69.0,-20.0,54.0,-48.0,61.0,1.0,-62.0,37.0,-77.0,87.0,45.0,96.0,-21.0],\n",
        "              [8.0,-16.0,40.0,-7.0,-57.0,91.0,-8.0,50.0,28.0,36.0,50.0,48.0,94.0,55.0,-80.0,82.0,13.0,69.0,62.0,87.0],\n",
        "              [-15.0,-79.0,-4.0,-31.0,10.0,32.0,-20.0,24.0,-86.0,-32.0,43.0,92.0,-98.0,-67.0,47.0,92.0,-33.0,-99.0,81.0,91.0],\n",
        "              [-97.0,60.0,-77.0,64.0,81.0,-66.0,-46.0,86.0,-82.0,-97.0,64.0,66.0,-19.0,76.0,-2.0,-71.0,-16.0,-98.0,86.0,-73.0],\n",
        "              [-14.0,77.0,-67.0,-76.0,-44.0,8.0,27.0,-96.0,-41.0,-44.0,-85.0,-34.0,62.0,-65.0,55.0,-36.0,76.0,18.0,4.0,12.0],\n",
        "              [15.0,70.0,2.0,86.0,41.0,1.0,20.0,83.0,-35.0,71.0,39.0,-27.0,6.0,44.0,65.0,27.0,-83.0,61.0,-71.0,67.0],\n",
        "              [97.0,-39.0,57.0,-34.0,85.0,55.0,36.0,53.0,53.0,-60.0,-5.0,1.0,0.0,-6.0,76.0,1.0,61.0,90.0,-93.0,53.0],\n",
        "              [-16.0,-30.0,69.0,5.0,-74.0,-95.0,43.0,52.0,-58.0,-75.0,-69.0,73.0,87.0,99.0,-65.0,69.0,-23.0,97.0,91.0,-27.0],\n",
        "              [-40.0,83.0,28.0,-10.0,-20.0,-94.0,68.0,-23.0,-45.0,14.0,-87.0,-54.0,-91.0,26.0,-23.0,100.0,-20.0,-79.0,87.0,12.0],\n",
        "              [46.0,65.0,28.0,-92.0,-9.0,-5.0,19.0,-94.0,-69.0,51.0,-27.0,-35.0,-96.0,-81.0,47.0,-9.0,58.0,49.0,28.0,-93.0],\n",
        "              [-55.0,26.0,-58.0,53.0,56.0,-86.0,-47.0,-11.0,90.0,-71.0,25.0,9.0,22.0,18.0,90.0,-21.0,-18.0,-84.0,-64.0,-64.0],\n",
        "              [43.0,94.0,-45.0,-59.0,33.0,-43.0,96.0,92.0,-14.0,16.0,-72.0,-7.0,-65.0,15.0,10.0,-64.0,70.0,-56.0,46.0,68.0],\n",
        "              [-31.0,-93.0,-61.0,38.0,56.0,-100.0,-13.0,27.0,-91.0,-74.0,64.0,-86.0,25.0,-62.0,-99.0,-46.0,-28.0,-13.0,-10.0,39.0],\n",
        "              [29.0,94.0,60.0,21.0,-49.0,0.0,94.0,5.0,11.0,-95.0,-63.0,8.0,-10.0,68.0,-86.0,-82.0,-69.0,21.0,38.0,61.0],\n",
        "              [-96.0,41.0,-59.0,79.0,-15.0,62.0,57.0,-67.0,61.0,-65.0,7.0,-90.0,26.0,-71.0,-60.0,48.0,-28.0,-6.0,-63.0,22.0],\n",
        "              [-95.0,42.0,87.0,28.0,21.0,33.0,58.0,-30.0,54.0,-10.0,-56.0,48.0,-39.0,-4.0,-76.0,-20.0,58.0,-57.0,75.0,-34.0],\n",
        "              [-88.0,-30.0,28.0,70.0,-60.0,-58.0,-84.0,-74.0,-29.0,73.0,-98.0,36.0,-37.0,74.0,-62.0,83.0,-29.0,11.0,-67.0,-24.0],\n",
        "              [88.0,-82.0,-26.0,38.0,-31.0,49.0,47.0,-82.0,-72.0,6.0,51.0,-0.0,-31.0,-63.0,4.0,27.0,-38.0,-91.0,44.0,-26.0],\n",
        "              [-35.0,-89.0,-34.0,68.0,-25.0,81.0,-26.0,69.0,-22.0,5.0,9.0,43.0,-81.0,-55.0,-69.0,81.0,-79.0,-14.0,-13.0,-62.0],\n",
        "              [95.0,-0.0,48.0,-11.0,-8.0,-46.0,-21.0,-78.0,96.0,99.0,50.0,-25.0,-0.0,-87.0,39.0,-68.0,-4.0,-89.0,48.0,-9.0],\n",
        "              [8.0,42.0,46.0,42.0,-78.0,-73.0,41.0,6.0,60.0,18.0,18.0,82.0,63.0,73.0,-79.0,-60.0,-38.0,43.0,-69.0,-92.0,]])\n",
        "b = np.array([ 780, -210, -690,  190,  850,  720, -900,  -70,  790, -390, -520, -610, -380, -950,  100, -820, -500,  40,  400,  300,  800,  630,  -10, -290,  480,  540, -930,  360,  560,  -990])\n",
        "c0 = np.array([93, 36, 46, 13, 17, 71, 92, 69, 68,  4, 14, 80, 65,  6,  3, 10, 40, 52, 99, 58])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxtzk1JMXZ0v"
      },
      "outputs": [],
      "source": [
        "# solve LP(c_0)\n",
        "your code here\n",
        "\n",
        "# solve GIO(x0)\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfWbiRfmSE2n"
      },
      "outputs": [],
      "source": [
        "# generate 10 samples\n",
        "your code here\n",
        "\n",
        "# solve GIO to recover c_hat_10\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYtIymDcSa8X"
      },
      "outputs": [],
      "source": [
        "# repeat the same example for n = 100, n = 500, and compare the result.\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVTKn9zcSni3"
      },
      "outputs": [],
      "source": [
        "# plot the distance = ||c_hat - c_hat_n||_2 against the number of samples, range from N=1 to N=1,000 in steps of 50\n",
        "your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suhTDN8zV5YP"
      },
      "source": [
        "<font color=\"green\">Your answer:</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtRMAS4MV4vB"
      },
      "source": [
        "##References\n",
        "[1] Timothy CY Chan, Taewoo Lee, and Daria Terekhov. Inverse optimization: Closed-form solutions, geometry, and goodness of fit. Management Science, 65(3):1115–1135, 2019."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
